# 基于Single-Pass 的事件聚类

## 1、现状

目前天巡舆情采用Single-Pass算法实现事件聚类

## 2、具体实现的细节和优化的点

Single-Pass 是一个很常规的过程，也是业内比较常规的一个实现方式，我们在尝试第一版之后 会发现效果不是那么的理想。因此我们使用了一些技术包括：

### 2.1、对TF-IDF的细节及其应用（前置处理）

#### 2.1.1、基本理解概念理解：

[生动理解TF-IDF算法]: https://zhuanlan.zhihu.com/p/31197209

TF->根据词在文章中的词频率计算，可以直接算。
![TF.png](images%2FTF.png)
IDF->为了强调在一个可以表明词的“常见性”或者说“重要程度”的一个指标
![IDF.png](images%2FIDF.png)



#### 2.1.2、我们的实现：

在对每个文章的分词（分完词），计算每个分词的TF—IDF ，得到空间向量，它也可以做为“这个文章的特征”
![tf-idf过程.png](images%2Ftf-idf%E8%BF%87%E7%A8%8B.png)


#### 2.1.3、值得注意的点：

1、语料库需要保持不变，是提前计算好的，这个语料库是从我们几千万的数据库拿的文章，这个是一直不变的，这样子计算出来的IDF才是更准确的的，简单来说，对于两个词，需要分母的计算规则是一样的

2、在计算语料库的过程中，我们不可能对整个语料库的所有词，做TF-IDF，那样子量太大了，我们在语料库中保留N个对我们有意义的词。



#### 2.1.4、存在问题：

可能会失帧？

是的，可能会有。正常来说，每次来一批文章，IDF应该在N个文章算。但是我们这么做有我们的两个原因和考虑：

1、适应场景：

我们的场景是不断有新数据进来，新数据一个一个进来，我们不可能等数据凑成一批再给他们算，所以来一个算一个的过程中，我们就必须把语料库子提取给抽象出来，IDF对于每一个新进来的数据都是可以直接算的，也是结合我们场景的无奈之举。

2、灵活配置：

这样子做其实还有另一个好处，我们这样子做也不需要做排除词，我们做了留存词表。

以后每个数据进来，都会找它分完的词，是否在我们的留存词表里。不存在的词我们不考虑。

我们可以很较为灵活的定义这些留存词表，比如我们定义 汽车行业相关的，那新“汽车”这个词非常常见，我们只需要拿一批汽车行业的语料库，就可能可以得到更准确的IDF。

3、更好地抽象：

假设我们有1w个保留词，我们每个文章，都可以通过判断分词结果有没有在这1w个保留词里。如果计算得到一个空间向量，然后通过向量余弦相似，计算距离，距离，就表示重复相似度。这个过程也是我们聚类的前置处理，得到的是一个空间向量。



### 2.2、Single-Pass全流程

#### 2.2.1具体的数据流程如下：

1、初始化：用于限定单次遍历的数据量。设置簇的容量上限以及其他相关参数。
2、读取新数据：从数据流中读取最新的一批数据，这些数据是实时产生的舆情数据。

3、特征提取：对每条新数据进行特征提取，使用 tf-idf 等技术将文本数据转换为数值表示。

4、聚类处理：对提取后的特征进行聚类操作。首先，将新数据与已有的聚类簇进行相似度计算，使用欧氏距离、余弦相似度等度量方式。然后，根据相似度阈值或其他条件，判断是否将新数据归入某个现有簇，或者创建新的簇来容纳该数据。如果某个簇已达到容量上限，则可以考虑采用替换策略，将较不重要的数据替换出去。

5、更新结果：根据聚类结果进行相应的更新和记录，包括生成聚类簇的中心点、簇的大小、簇内的样本等信息。使用各种数据结构（可以使用哈希表、堆、优先队列等，但是我们目前只是使用了数组来保存）来维护和存储聚类结果。

6、重复步骤2-5：不断循环以上步骤，读取新数据，并进行特征提取、聚类处理以及结果更新。这样就能实现对实时数据的持续处理和动态聚类。

![事件聚类算法-1.png](images%2F%E4%BA%8B%E4%BB%B6%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-1.png)



##### 2.2.2、一些具体的代码+注释，补充说明各个指标：

```python
def single_pass(self, data, force_merge: bool = False):
    # 重置所有数据结构以便处理新数据
    self.reset_data()
    
    # 将输入数据切分成句子
    cut_tokens = self.cut_sentences(data)
    
    # 计算文本和标题的 TF-IDF 向量
    tf_idfs = self.get_tfidf(cut_tokens)

    # 初始化用于跟踪簇的变量
    _current_cluster_idx = 0
    origin_cluster_id = []

    # 开始遍历数据
    for _text_idx, data_item in enumerate(data):
        # 如果不存在簇或者未请求强制合并，初始化第一个簇或者基于输入创建簇
        if _current_cluster_idx == 0 or (len(str(data_item[3] or '')) > 0 and not force_merge):
            # 添加当前文本和标题的 TF-IDF 向量作为簇中心
            self.cluster_center_text_vec.append(tf_idfs[0][_text_idx])
            self.cluster_center_title_vec.append(tf_idfs[1][_text_idx])
            
            # 记录当前簇的文本索引
            self.cluster_2_idx[_current_cluster_idx] = [_text_idx]
            
            # 如果输入数据中包含簇信息且未请求强制合并，则使用输入的簇标识
            if len(str(data_item[3] or '')) > 0 and not force_merge:
                origin_cluster_id.append(str(data_item[3]))
            else:
                # 否则，生成新的簇标识
                origin_cluster_id.append(get_cluster_id())
            
            _current_cluster_idx += 1
        else:
            # 与现有簇进行相似度比较
            max_simi, _max_idx = self.cosine_simi([tf_idfs[0][_text_idx], tf_idfs[1][_text_idx]])
            
            # 如果相似度超过阈值，将当前文本添加到现有簇
            if max_simi >= self.simi_thr:
                self.cluster_2_idx[_max_idx].append(_text_idx)
            else:
                # 如果不相似，则创建新的簇
                self.cluster_center_text_vec.append(tf_idfs[0][_text_idx])
                self.cluster_center_title_vec.append(tf_idfs[1][_text_idx])
                self.cluster_2_idx[_current_cluster_idx] = [_text_idx]
                _current_cluster_idx += 1
                origin_cluster_id.append(get_cluster_id())

    # 构建最终的聚类结果
    out_cluster = {}
    for _cluster_idx, _cluster_text_idx_list in self.cluster_2_idx.items():
        out_cluster[origin_cluster_id[_cluster_idx]] = []
        for _text_idx in _cluster_text_idx_list:
            out_cluster[origin_cluster_id[_cluster_idx]].append(self.origin_idx_list[_text_idx])
    
    return out_cluster

```

我们余弦相似度来计算相似

然后在计算余弦相似度时，将标题和全文的相似度进行了加权处理，如果标题的相似度大于0.9，则增加0.3的权重，如果标题的相似度小于0.15，则减少0.3的权重。

简直而言之，文章标题和文章内容的权重分别单独计算

```python
    def cosine_simi(self, vec):
        simi_text = cosine_similarity(np.array([vec[0]]), np.array(self.cluster_center_text_vec))
        simi_title = cosine_similarity(np.array([vec[1]]), np.array(self.cluster_center_title_vec))
        simi = simi_text + np.where(simi_title > 0.9, 0.3, np.where(simi_title < 0.15, -0.3, 0))
        max_idx = np.argmax(simi[0], axis=0)
        max_val = simi[0][max_idx]
        return max_val, max_idx

```



ps:我们目前相似的判定阈值为0.85，没有采用一些实验来判断聚类效果，纯人工测试，后续可以补充，目前够用。





### 2.3、word2vec模型：

#### 存在问题：

通过上面的处理，原有的舆情事件聚类效果一般，仅基于tf-idf进行文章相似度比较，导致只能将用词几乎一样的文章识别为相似，漏检率高。

#### word2vec模型是什么：

简单理解，word2vec模型，提前把世界上所有的词都提前算好，得到一个200-400维的向量。这个向量是跟语意相关的。

原理：在大规模的文章中，国王、往后同时出现的频率高，那他们算出来的向量就非常接近。

基本上所有词都可以通过上面的模型计算得到一个向量。

#### 做法：

我们的本质还是需要从 词的TF- IDF、词向量，转化为句向量，这么做可以把相似度从原本的字面相似度，转为

字面相似度、语意相似度相结合的一个相似度 ，对底层来说，就是一个更准确的向量

我们通过word2vec，得到每个词的向量，我们的最后结果，还是需要得到一个1w纬的向量。

1、词向量叠加

2、把TF-IDF作为一个权重

3、词向量叠加结果乘以TF-IDF

成为文章的句向量。

#### 代码：

```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

class Word2VecSinglePassClass:
    def __init__(self, embedding_model, simi_thr=0.8):
        self.embedding = embedding_model  # Word2Vec或其他嵌入模型
        self.vectorizer = TfidfVectorizer()  # TF-IDF向量化器
        self.simi_thr = simi_thr  # 用于聚类的相似性阈值
        self.cluster_center_vec = []  # 用于存储聚类中心向量的列表

    def reset_data(self):
        # 重置任何必要的数据结构或变量
        pass

    def cut_sentences(self, data):
        # 在这里实现分词操作，对于中文文本可以使用分词工具，将每个文本切分成词语或标记
        # 返回切分后的结果，可能是一个包含词语或标记的列表
        pass

    def get_tfidf(self, cut_tokens):
        # 使用TF-IDF向量化器对切分后的文本进行TF-IDF计算
        # 返回TF-IDF矩阵或其他表示文本权重的数据结构
        pass

    def get_doc_vec(self, matrix):
        # 根据TF-IDF矩阵计算文档向量，结合Word2Vec嵌入模型
        # 返回所有文档向量的列表
        
        """
        计算文档向量，通过将每个词的Word2Vec嵌入值与TF-IDF值相乘，然后累加得到整个文档的向量。

        Args:
            matrix (list): 包含TF-IDF权重的文档矩阵。

        Returns:
            list: 包含所有文档向量的列表。
        """
        voc = self.vectorizer.get_feature_names_out()
        all_doc_vec = []
        for index, doc in enumerate(matrix):
            doc_vec = np.zeros(shape=(200,), dtype=np.float32)
            for i, word_tfidf in enumerate(doc):
                if word_tfidf != 0:
                    try:
                        doc_vec += self.embedding[voc[i]] * word_tfidf
                    except KeyError:
                        # 如果词在Word2Vec嵌入模型中不存在，忽略该词
                        pass
            all_doc_vec.append(doc_vec)
            # 可以考虑添加对文档向量的平均处理
        return all_doc_vec

    def cosine_simi(self, doc_vec, filtered_cluster_indexes):
        # 计算给定文档向量与一组聚类中心向量的余弦相似度
        # 返回最大相似度和对应的聚类中心索引
        pass

    def word2vec_singelpass(self, data, force_merge: bool = False):
       """
        :param data: id_xxx, text_xxx, title_xxx, cluster_id_xxx, [filtered_cluster_id_xxx]
        :param force_merge:
        :return:
        """
        self.reset_data()
        cut_tokens = self.cut_sentences(data)
        tf_idfs = self.get_tfidf(cut_tokens)
        # 用累加词向量来表示某一篇文章
        all_doc_vec = self.get_doc_vec(tf_idfs)

        # 两两计算文章之间的余弦相似度
        all_cluster = [] #[[cluster_center_id, id, ...],[],[],...]
        cluster_id = []


        for index, d in enumerate(data):
            id = d[0]
            if len(all_cluster) == 0 or (d[3] and not force_merge):
                self.cluster_center_vec.append(all_doc_vec[index])
                all_cluster.append([id])
                if d[3] and not force_merge:
                    cluster_id.append(int(d[3]))
                else:
                    cluster_id.append(get_cluster_id())
            else:
                # 过滤运营确认不正确的聚类结果
                filtered_cluster_indexes = []
                if d[4]:
                    for cluster_index, cluster_id in enumerate(cluster_id):
                        if cluster_id in d[4]:
                            filtered_cluster_indexes.append(cluster_index)
                max_simi, _max_idx = self.cosine_simi(all_doc_vec[index], filtered_cluster_indexes)
                if max_simi >= self.simi_thr:
                    all_cluster[_max_idx].append(id)
                else:
                    all_cluster.append([id])
                    self.cluster_center_vec.append(all_doc_vec[index])
                    cluster_id.append(get_cluster_id())

        out_cluster = {}
        for index, c_id in enumerate(cluster_id):
            out_cluster[c_id] = all_cluster[index]

        return out_cluster

```





### 2.4、分词上的优化：
我们考虑使用一些NIP的模型做分词，但是后来弃用了，因为我们的业务场景不合适；对于舆情的数据，可能是一片很长的文章，如果用这些更高级的分词模型，速度不够快，严重影响性能。因此我们还是使用的jieba分词。



### 2.5、技术选型：


技术选型是在项目中选择合适的算法和工具的过程，市面上流行的有：k-means和single-pass，这是两种不同的聚类算法。

1. **K-means算法：**
   - 优势：
     - 简单而直观，易于实现和理解。
     - 在大规模数据集上具有较好的效果。
   - 适用场景：
     - 数据集具有明显的聚类结构，簇之间距离较明显。
     - 聚类的形状是凸的。
   - 注意事项：
     - 对异常值敏感，可能会影响聚类结果。
     - 需要提前指定簇的数量（k值）。
2. **Single-pass算法：**
   - 优势：
     - 适用于数据流式处理，适合大规模数据集。
     - 不需要提前知道簇的数量。
   - 适用场景：
     - 数据集可能在不断变化，需要实时处理。
     - 不方便一次性加载整个数据集到内存中。
   - 注意事项：
     - 对初始簇的选择敏感，可能影响聚类效果。
     - 适用于对计算资源有限的情况。

**综合考虑：**

- Single-pass的选择是因为数据是动态流式的，而且需要进行实时的聚类处理。





## 总结

我们的舆情聚类算法采用Single-Pass， 还覆盖了很多前置操作，我们事件聚类的本质就把文本提取出特征，将相似的文章合并起来的Single-Pass 只是最后的一个过程。

先讲讲怎么获取文章特征，
首先我们会分词，分词这块也尝试用一些NIP的高级分词，但是后面发现影响效率，对总体效果影响也不大，后面还是使用简单快速的jieba。
分词完我们需要得到文章的一个抽象概念，就是向量，他可以表示一个文章的特征。

这就要理解TF-IDF的概念，TF就是文章词频率，但是IDF 需要有一个基数，也是是文章库。
我们现在的文章库是从我们数据库里的数据集提取出来的，我们首先对整个库做TF–IDf，这可以提取出一些重要的词，作为我们的样本词。

举个例子，当一个文章分完词后，如果一个词在文章库中有，他的tf–idf就不为0，可以计算，我们对每个在词tf–idf，这作为一个向量。

除此之外，我们计算出来的TF-IDF，只反应文本的字面相似，我们通过采用word2vec的模型，对算法进行优化。

最后，向量计算余弦相似度，从而用single–pass方式做合并。